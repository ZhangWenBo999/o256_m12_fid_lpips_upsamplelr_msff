import torch
import torch.nn as nn
import torch.nn.functional as F
from einops import rearrange
Conv2d = nn.Conv2d
'''
DHSA:Restoring Images in Adverse Weather Conditions via Histogram Transformer (ECCV 2024)

èƒŒæ™¯ï¼šè¿™ç¯‡è®ºæ–‡æå‡ºäº†ä¸€ç§åä¸º Histoformer çš„æ–°å‹ Transformer æ¶æ„ï¼Œç”¨äºåœ¨æ¶åŠ£å¤©æ°”ï¼ˆå¦‚é›¨é›ªã€é›¾éœ¾ç­‰ï¼‰æ¡ä»¶ä¸‹æ¢å¤
å›¾åƒçš„è´¨é‡ã€‚ç ”ç©¶äººå‘˜åŸºäºè§‚å¯Ÿå‘ç°ï¼Œæ¶åŠ£å¤©æ°”å¼•å‘çš„é™è§£æ¨¡å¼ï¼ˆå¦‚äº®åº¦å˜åŒ–å’Œé®æŒ¡ï¼‰é€šå¸¸å­˜åœ¨ç›¸ä¼¼æ€§ï¼Œå› æ­¤æå‡ºäº†ç›´æ–¹å›¾
è‡ªæ³¨æ„åŠ›ï¼ˆHistogram Self-Attentionï¼‰æœºåˆ¶æ¥æœ‰æ•ˆå¤„ç†è¿™äº›é—®é¢˜ã€‚

DHSAæ¨¡å—åŸç†åŠä½œç”¨ï¼š
ä¸€. åŠ¨æ€èŒƒå›´å·ç§¯
åœ¨è¿›å…¥è‡ªæ³¨æ„åŠ›æœºåˆ¶ä¹‹å‰ï¼ŒDHSA å¯¹ç‰¹å¾è¿›è¡ŒåŠ¨æ€èŒƒå›´çš„å·ç§¯å¤„ç†ã€‚è¿™éƒ¨åˆ†æ“ä½œä¼šå…ˆå°†ç‰¹å¾å›¾æŒ‰åƒç´ å¼ºåº¦æ°´å¹³å’Œå‚ç›´æ’åºï¼Œ
å†ä½¿ç”¨å·ç§¯ã€‚
1. åˆ†æ”¯åˆ†ç¦»ï¼šè¾“å…¥ç‰¹å¾å›¾ğ¹è¢«æ²¿é€šé“ç»´åº¦åˆ†ä¸ºä¸¤ä¸ªåˆ†æ”¯ã€‚F1å’ŒF2
2. æ’åºæ“ä½œï¼šå¯¹ğ¹1è¿›è¡Œæ°´å¹³å’Œå‚ç›´æ’åºï¼Œé‡æ–°æ’åˆ—åƒç´ ï¼Œä½¿å¾—é«˜å¼ºåº¦å’Œä½å¼ºåº¦çš„åƒç´ é›†ä¸­åœ¨çŸ©é˜µçš„è§’è½ï¼Œä»è€Œå¢å¼ºå·ç§¯
åœ¨é™è§£åŒºåŸŸä¸Šçš„ä½œç”¨ã€‚
3. å·ç§¯æ“ä½œï¼šå°†æ’åºåçš„ç‰¹å¾å’ŒåŸå§‹ç‰¹å¾ğ¹2è¿æ¥ï¼ˆconcatï¼‰èµ·æ¥ï¼Œé€šè¿‡ä¸€ä¸ªæ·±åº¦å¯åˆ†ç¦»å·ç§¯ï¼Œæå–åŠ¨æ€èŒƒå›´å†…çš„ç©ºé—´ç‰¹å¾ã€‚
åŸå› 1ï¼šå¢å¤§å·ç§¯æ„Ÿå—é‡ï¼Œæ•æ‰é•¿è·ç¦»ä¾èµ–å…³ç³»
ä¼ ç»Ÿå·ç§¯é€šå¸¸å…·æœ‰å›ºå®šçš„æ„Ÿå—é‡ï¼Œä¸»è¦å…³æ³¨å±€éƒ¨é‚»åŸŸçš„ä¿¡æ¯ï¼Œè€ŒåŠ¨æ€èŒƒå›´å·ç§¯é€šè¿‡å¯¹åƒç´ æ’åºï¼Œä½¿å¾—å·ç§¯æ ¸å¯ä»¥è¦†ç›–å…·æœ‰ç›¸ä¼¼å¼ºåº¦
ä½†åœ¨ç©ºé—´ä¸Šè·ç¦»è¾ƒè¿œçš„åƒç´ ã€‚
åŸå› 2ï¼šå¼ºåŒ–é™è§£åŒºåŸŸä¸èƒŒæ™¯åŒºåŸŸçš„åŒºåˆ†
åœ¨æ¶åŠ£å¤©æ°”æ¡ä»¶ä¸‹ï¼Œå›¾åƒä¸­çš„é™è§£åŒºåŸŸï¼ˆå¦‚é›¨æ»´ã€é›¾éœ¾ç­‰ï¼‰å’ŒèƒŒæ™¯åŒºåŸŸçš„åƒç´ å¼ºåº¦åˆ†å¸ƒå¾€å¾€ä¸åŒã€‚åŠ¨æ€èŒƒå›´å·ç§¯å…ˆå¯¹åƒç´ è¿›è¡Œæ’åºï¼Œ
ä½¿å¾—é™è§£åŒºåŸŸå’ŒèƒŒæ™¯åŒºåŸŸçš„åƒç´ åˆ†å¸ƒæ›´ä¸ºé›†ä¸­ï¼Œä¾¿äºå·ç§¯æ ¸åˆ†åˆ«å¤„ç†é™è§£ä¿¡æ¯å’Œä¿ç•™èƒŒæ™¯ä¿¡æ¯ã€‚
äºŒ. ç›´æ–¹å›¾è‡ªæ³¨æ„åŠ›
ä¼ ç»Ÿçš„ Transformer è‡ªæ³¨æ„åŠ›é€šå¸¸æ˜¯åœ¨å›ºå®šç©ºé—´èŒƒå›´æˆ–é€šé“ç»´åº¦å†…è®¡ç®—ï¼Œé™åˆ¶äº†å¯¹é•¿è·ç¦»ç‰¹å¾çš„å»ºæ¨¡èƒ½åŠ›ã€‚è€ŒDHSAå¼•å…¥ç›´
æ–¹å›¾è‡ªæ³¨æ„åŠ›æœºåˆ¶æ¥è§£å†³è¿™ä¸€é—®é¢˜ã€‚
1. åˆ†ç»„binæ“ä½œï¼šDHSA æ ¹æ®åƒç´ çš„å¼ºåº¦å¯¹ç‰¹å¾è¿›è¡Œæ’åºå¹¶åˆ†ç»„ä¸ºå¤šä¸ªbinï¼Œæ¯ä¸ªbinåŒ…å«å…·æœ‰ç›¸ä¼¼å¼ºåº¦çš„åƒç´ ã€‚
2. Bin-wise Histogram Reshapingï¼ˆBHRï¼‰:å°†å›¾åƒç‰¹å¾åˆ†é…åˆ°ä¸åŒ bin ä¸­ï¼Œæ¯ä¸ª bin è¦†ç›–æ›´å¤šåƒç´ ï¼Œç”¨äºæå–å¤§å°ºåº¦çš„å…¨å±€ä¿¡æ¯ã€‚
3. Frequency-wise Histogram Reshapingï¼ˆFHRï¼‰ï¼šå°† bin è®¾ä¸ºä¸åŒé¢‘ç‡çš„åƒç´ åˆ†å¸ƒï¼Œæ¯ä¸ª bin åªåŒ…å«å°‘é‡åƒç´ ï¼Œä»è€Œæå–ç»†ç²’
åº¦çš„å±€éƒ¨ä¿¡æ¯ã€‚
ä¸‰. è‡ªæ³¨æ„åŠ›è®¡ç®—
1. æŸ¥è¯¢-é”®-å€¼æ’åºï¼šä½¿ç”¨é‡ç»„åçš„ bin è¿›è¡ŒæŸ¥è¯¢ï¼ˆQueryï¼‰å’Œé”®ï¼ˆKeyï¼‰çš„æ’åºï¼Œä¿è¯ç›¸åŒå¼ºåº¦çš„åƒç´ è¢«åˆ†é…åˆ°ç›¸åŒçš„binä¸­ã€‚
2. è‡ªæ³¨æ„åŠ›è®¡ç®—ï¼šåœ¨ BHR å’Œ FHR çš„ç‰¹å¾ä¸Šåˆ†åˆ«è®¡ç®—æ³¨æ„åŠ›ï¼Œå¹¶å°†ä¸¤è€…çš„ç»“æœé€šè¿‡é€å…ƒç´ ä¹˜æ³•èåˆï¼Œç”Ÿæˆæœ€ç»ˆçš„æ³¨æ„åŠ›å›¾ã€‚
è¿™ç§æ–¹æ³•ç¡®ä¿äº†æ¯ä¸ªbinéƒ½èƒ½åœ¨é€‚å½“çš„å°ºåº¦ä¸Šå…³æ³¨é™è§£ä¿¡æ¯ï¼Œä½¿æ¨¡å‹åœ¨å¤©æ°”é™è§£æ¨¡å¼ä¸­èƒ½å¤Ÿæ•æ‰åˆ°å…·æœ‰ç›¸ä¼¼å¼ºåº¦çš„åƒç´ é—´çš„å…³è”å…³ç³»
ï¼Œæå‡äº†é•¿è·ç¦»å’Œå±€éƒ¨ç‰¹å¾çš„å»ºæ¨¡æ•ˆæœã€‚

å››ã€é€‚ç”¨åœºæ™¯ï¼šå›¾åƒæ¢å¤ï¼Œå›¾åƒå»å™ªã€é›¨ã€é›ªã€é›¾ï¼Œç›®æ ‡æ£€æµ‹ï¼Œå›¾åƒå¢å¼ºç­‰æ‰€æœ‰CV2äºŒç»´ä»»åŠ¡é€šç”¨ã€‚
'''
## Dynamic-range Histogram Self-Attention (DHSA)
# åŠ¨æ€èŒƒå›´ç›´æ–¹å›¾è‡ªæ³¨æ„åŠ›
# ä¼ ç»Ÿçš„Transformerè‡ªæ³¨æ„åŠ›é€šå¸¸æ˜¯åœ¨å›ºå®šç©ºé—´èŒƒå›´æˆ–é€šé“ç»´åº¦å†…è¿›è¡Œè®¡ç®—ï¼Œé™åˆ¶äº†å¯¹é•¿è·ç¦»ç‰¹å¾çš„å»ºæ¨¡èƒ½åŠ›ã€‚
class DHSA(nn.Module):
    def __init__(self, dim, num_heads=4, bias=False, ifBox=True, scales=[3, 5, 7]):
        super(DHSA, self).__init__()
        self.factor = num_heads
        self.ifBox = ifBox
        self.num_heads = num_heads
        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))

        self.qkv = Conv2d(dim, dim * 5, kernel_size=1, bias=bias)
        self.qkv_dwconv357 = nn.ModuleList([
            Conv2d(dim * 5, dim * 5, kernel_size=scale, stride=1, padding=scale // 2, groups=dim * 5, bias=bias)
            for scale in scales
        ])
        self.fuse_dwconv357 = nn.Conv2d(dim * 5 * len(scales), dim * 5, kernel_size=1)
        self.project_out = Conv2d(dim, dim, kernel_size=1, bias=bias)

    def pad(self, x, factor):
        hw = x.shape[-1]
        t_pad = [0, 0] if hw % factor == 0 else [0, (hw // factor + 1) * factor - hw]
        x = F.pad(x, t_pad, 'constant', 0)
        return x, t_pad

    def unpad(self, x, t_pad):
        _, _, hw = x.shape
        return x[:, :, t_pad[0]:hw - t_pad[1]]

    def softmax_1(self, x, dim=-1):
        logit = x.exp()
        logit = logit / (logit.sum(dim, keepdim=True) + 1)
        return logit

    def normalize(self, x):
        mu = x.mean(-2, keepdim=True)
        sigma = x.var(-2, keepdim=True, unbiased=False)
        return (x - mu) / torch.sqrt(sigma + 1e-5)  # * self.weight + self.bias

    def reshape_attn(self, q, k, v, ifBox):
        b, c = q.shape[:2]
        q, t_pad = self.pad(q, self.factor)
        k, t_pad = self.pad(k, self.factor)
        v, t_pad = self.pad(v, self.factor)
        hw = q.shape[-1] // self.factor
        shape_ori = "b (head c) (factor hw)" if ifBox else "b (head c) (hw factor)"
        shape_tar = "b head (c factor) hw"
        q = rearrange(q, '{} -> {}'.format(shape_ori, shape_tar), factor=self.factor, hw=hw, head=self.num_heads)
        k = rearrange(k, '{} -> {}'.format(shape_ori, shape_tar), factor=self.factor, hw=hw, head=self.num_heads)
        v = rearrange(v, '{} -> {}'.format(shape_ori, shape_tar), factor=self.factor, hw=hw, head=self.num_heads)
        q = torch.nn.functional.normalize(q, dim=-1)
        k = torch.nn.functional.normalize(k, dim=-1)
        attn = (q @ k.transpose(-2, -1)) * self.temperature
        attn = self.softmax_1(attn, dim=-1)
        out = (attn @ v)
        out = rearrange(out, '{} -> {}'.format(shape_tar, shape_ori), factor=self.factor, hw=hw, b=b,
                        head=self.num_heads)
        out = self.unpad(out, t_pad)
        return out

    def forward(self, x):
        #
        b, c, h, w = x.shape
        # ä»¥ h è¿›è¡Œæ’åº
        x_sort, idx_h = x[:, :c // 2].sort(-2)
        # ä»¥ w è¿›è¡Œæ’åº
        x_sort, idx_w = x_sort.sort(-1)
        x[:, :c // 2] = x_sort
        # 1Ã—1 --> 3Ã—3
        qkv_dwconv = [qkv_dwconv357(self.qkv(x)) for qkv_dwconv357 in self.qkv_dwconv357]
        qkv = self.fuse_dwconv357(torch.cat(qkv_dwconv, dim=1))
        q1, k1, q2, k2, v = qkv.chunk(5, dim=1)  # b,c,x,x

        v, idx = v.view(b, c, -1).sort(dim=-1)
        q1 = torch.gather(q1.view(b, c, -1), dim=2, index=idx)
        k1 = torch.gather(k1.view(b, c, -1), dim=2, index=idx)
        q2 = torch.gather(q2.view(b, c, -1), dim=2, index=idx)
        k2 = torch.gather(k2.view(b, c, -1), dim=2, index=idx)

        # ä½¿ç”¨é‡ç»„åçš„binè¿›è¡ŒæŸ¥è¯¢å’Œé”®çš„æ’åºï¼Œä¿è¯ç›¸åŒå¼ºåº¦çš„åƒç´ è¢«åˆ†é…åˆ°ç›¸åŒçš„binä¸­ã€‚
        # BHR:å°†å›¾åƒç‰¹å¾åˆ†é…åˆ°ä¸åŒçš„binä¸­ï¼Œæ¯ä¸ªbinè¦†ç›–æ›´å¤šçš„åƒç´ ï¼Œç”¨äºæå–å¤§å°ºåº¦çš„å…¨å±€ä¿¡æ¯ã€‚
        out1 = self.reshape_attn(q1, k1, v, True)

        # FHR:å°†binè®¾ä¸ºä¸åŒé¢‘ç‡çš„åƒç´ åˆ†å¸ƒï¼Œæ¯ä¸ªbinä¸­åªåŒ…å«å°‘é‡åƒç´ ï¼Œä»è€Œæå–ç»†ç²’åº¦çš„å±€éƒ¨ä¿¡æ¯ã€‚
        out2 = self.reshape_attn(q2, k2, v, False)

        out1 = torch.scatter(out1, 2, idx, out1).view(b, c, h, w)
        out2 = torch.scatter(out2, 2, idx, out2).view(b, c, h, w)
        # åœ¨BHRå’ŒFHRçš„ç‰¹å¾ä¸Šåˆ†åˆ«è®¡ç®—æ³¨æ„åŠ›ï¼Œå¹¶å°†ä¸¤è€…çš„ç»“æœé€šè¿‡é€å…ƒç´ ä¹˜æ³•èåˆï¼Œç”Ÿæˆæœ€ç»ˆçš„æ³¨æ„åŠ›å›¾ã€‚
        # è¿™ç§æ–¹æ³•ç¡®ä¿äº†æ¯ä¸ªbinéƒ½èƒ½åœ¨é€‚å½“çš„å°ºåº¦ä¸Šå…³æ³¨é™è§£ä¿¡æ¯ï¼Œä½¿æ¨¡å‹åœ¨å¤©æ°”é™è§£æ¨¡å¼ä¸­èƒ½å¤Ÿæ•æ‰åˆ°å…·æœ‰ç›¸ä¼¼å¼ºåº¦çš„åƒç´ é—´çš„å…³è”å…³ç³»ï¼Œæå‡äº†é•¿è·ç¦»å’Œå±€éƒ¨ç‰¹å¾çš„å»ºæ¨¡æ•ˆæœã€‚
        out = out1 * out2
        out = self.project_out(out)
        out_replace = out[:, :c // 2]
        out_replace = torch.scatter(out_replace, -1, idx_w, out_replace)
        out_replace = torch.scatter(out_replace, -2, idx_h, out_replace)
        out[:, :c // 2] = out_replace
        return out

# è¾“å…¥ B C H W,  è¾“å‡ºB C H W
if __name__ == "__main__":
    # åˆ›å»ºDHSAæ¨¡å—çš„å®ä¾‹
    model = DHSA(64)
    input = torch.randn(1, 64, 128, 128)
    # æ‰§è¡Œå‰å‘ä¼ æ’­
    output = model(input)
    print('Input size:', input.size())
    print('Output size:', output.size())
